{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "sEWNtXl4PTQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJYUjGP2N7Lb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import sympy\n",
        "from sympy import symbols, series\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define symbol and functions\n",
        "x = symbols('x')\n",
        "functions = [\n",
        "    sympy.sin(x),\n",
        "    sympy.cos(x),\n",
        "    sympy.exp(x),\n",
        "    sympy.log(1+x),\n",
        "    sympy.tan(x),\n",
        "    sympy.sinh(x),\n",
        "    sympy.cosh(x),\n",
        "    sympy.sqrt(1+x)\n",
        "]\n",
        "\n",
        "def get_taylor_series(func, order=5):\n",
        "    # Returns Taylor series (up to x^(order-1)) as a string\n",
        "    return str(func.series(x, 0, order).removeO())\n",
        "\n",
        "# Create dataset: list of (function_str, taylor_series_str) pairs\n",
        "data_pairs = []\n",
        "for f in functions:\n",
        "    func_str = str(f)\n",
        "    taylor_str = get_taylor_series(f, order=5)\n",
        "    data_pairs.append((func_str, taylor_str))"
      ],
      "metadata": {
        "id": "jjCXa_nlN8iH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens and vocabulary creation\n",
        "special_tokens = ['<pad>', '<sos>', '<eos>']\n",
        "all_chars = set()\n",
        "for inp, out in data_pairs:\n",
        "    all_chars.update(list(inp))\n",
        "    all_chars.update(list(out))\n",
        "all_chars = sorted(list(all_chars))\n",
        "vocab = special_tokens + all_chars\n",
        "\n",
        "# Mapping from char to index and vice versa\n",
        "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "\n",
        "# Utility functions to encode/decode strings\n",
        "def encode_string(s, add_sos_eos=False):\n",
        "    tokens = []\n",
        "    if add_sos_eos:\n",
        "        tokens.append(char2idx['<sos>'])\n",
        "    tokens.extend([char2idx[ch] for ch in s])\n",
        "    if add_sos_eos:\n",
        "        tokens.append(char2idx['<eos>'])\n",
        "    return tokens\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    s = \"\"\n",
        "    for t in tokens:\n",
        "        ch = idx2char[t]\n",
        "        if ch == '<eos>':\n",
        "            break\n",
        "        if ch not in special_tokens:\n",
        "            s += ch\n",
        "    return s\n",
        "\n",
        "# Determine maximum lengths for source and target (including <sos>/<eos>)\n",
        "max_len_src = max(len(encode_string(inp, add_sos_eos=True)) for inp, _ in data_pairs)\n",
        "max_len_tgt = max(len(encode_string(tgt, add_sos_eos=True)) for _, tgt in data_pairs)"
      ],
      "metadata": {
        "id": "XYyCyy-bRAAj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaylorDataset(Dataset):\n",
        "    def __init__(self, data_pairs, max_len_src, max_len_tgt):\n",
        "        self.data_pairs = data_pairs\n",
        "        self.max_len_src = max_len_src\n",
        "        self.max_len_tgt = max_len_tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def pad_sequence(self, seq, max_len):\n",
        "        return seq + [char2idx['<pad>']] * (max_len - len(seq))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_str, tgt_str = self.data_pairs[idx]\n",
        "        src_seq = encode_string(inp_str, add_sos_eos=True)\n",
        "        tgt_seq = encode_string(tgt_str, add_sos_eos=True)\n",
        "        src_seq = self.pad_sequence(src_seq, self.max_len_src)\n",
        "        tgt_seq = self.pad_sequence(tgt_seq, self.max_len_tgt)\n",
        "        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)\n",
        "\n",
        "dataset = TaylorDataset(data_pairs, max_len_src, max_len_tgt)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "6AcMEV3_RBUU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define lstm , Transformer"
      ],
      "metadata": {
        "id": "T7iJPFvMRCk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LSTM-based Seq2Seq Model ---\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch, seq_len)\n",
        "        embedded = self.embedding(src)  # (batch, seq_len, embed_dim)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: (batch,) -> we unsqueeze to (batch, 1)\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)  # (batch, 1, embed_dim)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (batch, vocab_size)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        vocab_size = self.decoder.fc_out.out_features\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        # First input to decoder is <sos>\n",
        "        input = tgt[:, 0]\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "D1L0gx4OOq4b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer-based Seq2Seq Model ---\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].shape[1]])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_len, batch, d_model)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=8, num_encoder_layers=3,\n",
        "                 num_decoder_layers=3, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers,\n",
        "                                          num_decoder_layers, dim_feedforward, dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src, tgt: (batch, seq_len)\n",
        "        src = src.transpose(0, 1)  # (seq_len, batch)\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "\n",
        "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "        # Generate mask for target (for autoregressive decoding)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_emb.size(0)).to(tgt_emb.device)\n",
        "\n",
        "        output = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "        output = self.fc_out(output)\n",
        "        return output.transpose(0, 1)  # (batch, seq_len, vocab_size)"
      ],
      "metadata": {
        "id": "YIH-nZr2OvSo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(vocab)\n",
        "num_epochs = 50\n",
        "example_function = \"exp(x)\"\n",
        ""
      ],
      "metadata": {
        "id": "zMHK-hXpOyQ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "sVS1ppc9RLi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "encoder = EncoderLSTM(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
        "decoder = DecoderLSTM(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
        "model = Seq2SeqLSTM(encoder, decoder, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=char2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "print(\"Training LSTM model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt, teacher_forcing_ratio=0.5)\n",
        "        # output: (batch, seq_len, vocab_size); shift by one for target\n",
        "        loss = criterion(output[:,1:].reshape(-1, vocab_size), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[LSTM] Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "def predict_lstm(model, src_str, max_len=50):\n",
        "    model.eval()\n",
        "    src_seq = encode_string(src_str, add_sos_eos=True)\n",
        "    src_seq = src_seq + [char2idx['<pad>']] * (max_len_src - len(src_seq))\n",
        "    src_tensor = torch.tensor([src_seq], dtype=torch.long).to(device)\n",
        "    hidden, cell = model.encoder(src_tensor)\n",
        "    input_token = torch.tensor([char2idx['<sos>']], dtype=torch.long).to(device)\n",
        "    output_seq = [char2idx['<sos>']]\n",
        "    for i in range(max_len):\n",
        "        output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "        next_token = output.argmax(1).item()\n",
        "        output_seq.append(next_token)\n",
        "        if next_token == char2idx['<eos>']:\n",
        "            break\n",
        "        input_token = torch.tensor([next_token], dtype=torch.long).to(device)\n",
        "    return decode_tokens(output_seq)\n",
        "\n",
        "\n",
        "predicted = predict_lstm(model, example_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VcCqn3O2oK",
        "outputId": "7b9ccd1c-e6a3-4331-e72a-445840196593"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM model...\n",
            "[LSTM] Epoch 1/50, Loss: 3.3835\n",
            "[LSTM] Epoch 2/50, Loss: 2.7973\n",
            "[LSTM] Epoch 3/50, Loss: 2.3043\n",
            "[LSTM] Epoch 4/50, Loss: 2.0834\n",
            "[LSTM] Epoch 5/50, Loss: 1.9885\n",
            "[LSTM] Epoch 6/50, Loss: 1.8943\n",
            "[LSTM] Epoch 7/50, Loss: 1.7338\n",
            "[LSTM] Epoch 8/50, Loss: 1.6143\n",
            "[LSTM] Epoch 9/50, Loss: 1.5717\n",
            "[LSTM] Epoch 10/50, Loss: 1.6027\n",
            "[LSTM] Epoch 11/50, Loss: 1.4203\n",
            "[LSTM] Epoch 12/50, Loss: 1.5582\n",
            "[LSTM] Epoch 13/50, Loss: 1.3243\n",
            "[LSTM] Epoch 14/50, Loss: 1.2213\n",
            "[LSTM] Epoch 15/50, Loss: 1.1464\n",
            "[LSTM] Epoch 16/50, Loss: 1.1726\n",
            "[LSTM] Epoch 17/50, Loss: 1.1727\n",
            "[LSTM] Epoch 18/50, Loss: 1.0233\n",
            "[LSTM] Epoch 19/50, Loss: 1.0587\n",
            "[LSTM] Epoch 20/50, Loss: 0.9706\n",
            "[LSTM] Epoch 21/50, Loss: 0.9614\n",
            "[LSTM] Epoch 22/50, Loss: 0.9484\n",
            "[LSTM] Epoch 23/50, Loss: 0.8828\n",
            "[LSTM] Epoch 24/50, Loss: 0.8742\n",
            "[LSTM] Epoch 25/50, Loss: 0.8378\n",
            "[LSTM] Epoch 26/50, Loss: 0.8174\n",
            "[LSTM] Epoch 27/50, Loss: 0.7124\n",
            "[LSTM] Epoch 28/50, Loss: 0.7174\n",
            "[LSTM] Epoch 29/50, Loss: 0.5650\n",
            "[LSTM] Epoch 30/50, Loss: 0.6581\n",
            "[LSTM] Epoch 31/50, Loss: 0.5834\n",
            "[LSTM] Epoch 32/50, Loss: 0.5289\n",
            "[LSTM] Epoch 33/50, Loss: 0.5986\n",
            "[LSTM] Epoch 34/50, Loss: 0.5464\n",
            "[LSTM] Epoch 35/50, Loss: 0.5629\n",
            "[LSTM] Epoch 36/50, Loss: 0.5233\n",
            "[LSTM] Epoch 37/50, Loss: 0.4270\n",
            "[LSTM] Epoch 38/50, Loss: 0.4303\n",
            "[LSTM] Epoch 39/50, Loss: 0.3589\n",
            "[LSTM] Epoch 40/50, Loss: 0.3936\n",
            "[LSTM] Epoch 41/50, Loss: 0.3818\n",
            "[LSTM] Epoch 42/50, Loss: 0.3982\n",
            "[LSTM] Epoch 43/50, Loss: 0.3216\n",
            "[LSTM] Epoch 44/50, Loss: 0.3486\n",
            "[LSTM] Epoch 45/50, Loss: 0.3560\n",
            "[LSTM] Epoch 46/50, Loss: 0.2577\n",
            "[LSTM] Epoch 47/50, Loss: 0.2595\n",
            "[LSTM] Epoch 48/50, Loss: 0.2732\n",
            "[LSTM] Epoch 49/50, Loss: 0.2495\n",
            "[LSTM] Epoch 50/50, Loss: 0.2081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "9-lnjpNsPpC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=char2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training Transformer model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # For Transformer, feed the target sequence without the last token\n",
        "        output = model(src, tgt[:,:-1])\n",
        "        loss = criterion(output.reshape(-1, vocab_size), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Transformer] Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def predict_transformer(model, src_str, max_len=50):\n",
        "    model.eval()\n",
        "    src_seq = encode_string(src_str, add_sos_eos=True)\n",
        "    src_seq = src_seq + [char2idx['<pad>']] * (max_len_src - len(src_seq))\n",
        "    src_tensor = torch.tensor([src_seq], dtype=torch.long).to(device)\n",
        "    tgt_seq = [char2idx['<sos>']]\n",
        "    for i in range(max_len):\n",
        "        tgt_tensor = torch.tensor([tgt_seq], dtype=torch.long).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "        next_token = output[0, -1].argmax().item()\n",
        "        tgt_seq.append(next_token)\n",
        "        if next_token == char2idx['<eos>']:\n",
        "            break\n",
        "    return decode_tokens(tgt_seq)\n",
        "\n",
        "\n",
        "predicted = predict_transformer(model, example_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRIZHNuPn7n",
        "outputId": "bbd39b73-ca7b-45a7-dfdd-90e5b506fef7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Transformer model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Transformer] Epoch 1/50, Loss: 2.8935\n",
            "[Transformer] Epoch 2/50, Loss: 2.3734\n",
            "[Transformer] Epoch 3/50, Loss: 1.9930\n",
            "[Transformer] Epoch 4/50, Loss: 1.5652\n",
            "[Transformer] Epoch 5/50, Loss: 1.2854\n",
            "[Transformer] Epoch 6/50, Loss: 1.1796\n",
            "[Transformer] Epoch 7/50, Loss: 1.0491\n",
            "[Transformer] Epoch 8/50, Loss: 0.9328\n",
            "[Transformer] Epoch 9/50, Loss: 0.9101\n",
            "[Transformer] Epoch 10/50, Loss: 0.8805\n",
            "[Transformer] Epoch 11/50, Loss: 0.8725\n",
            "[Transformer] Epoch 12/50, Loss: 0.7806\n",
            "[Transformer] Epoch 13/50, Loss: 0.7737\n",
            "[Transformer] Epoch 14/50, Loss: 0.6951\n",
            "[Transformer] Epoch 15/50, Loss: 0.6877\n",
            "[Transformer] Epoch 16/50, Loss: 0.6432\n",
            "[Transformer] Epoch 17/50, Loss: 0.6488\n",
            "[Transformer] Epoch 18/50, Loss: 0.6777\n",
            "[Transformer] Epoch 19/50, Loss: 0.5730\n",
            "[Transformer] Epoch 20/50, Loss: 0.5659\n",
            "[Transformer] Epoch 21/50, Loss: 0.5614\n",
            "[Transformer] Epoch 22/50, Loss: 0.5877\n",
            "[Transformer] Epoch 23/50, Loss: 0.5296\n",
            "[Transformer] Epoch 24/50, Loss: 0.6700\n",
            "[Transformer] Epoch 25/50, Loss: 0.5685\n",
            "[Transformer] Epoch 26/50, Loss: 0.5130\n",
            "[Transformer] Epoch 27/50, Loss: 0.5460\n",
            "[Transformer] Epoch 28/50, Loss: 0.5428\n",
            "[Transformer] Epoch 29/50, Loss: 0.5603\n",
            "[Transformer] Epoch 30/50, Loss: 0.4411\n",
            "[Transformer] Epoch 31/50, Loss: 0.4627\n",
            "[Transformer] Epoch 32/50, Loss: 0.3894\n",
            "[Transformer] Epoch 33/50, Loss: 0.3973\n",
            "[Transformer] Epoch 34/50, Loss: 0.4003\n",
            "[Transformer] Epoch 35/50, Loss: 0.3787\n",
            "[Transformer] Epoch 36/50, Loss: 0.3891\n",
            "[Transformer] Epoch 37/50, Loss: 0.3841\n",
            "[Transformer] Epoch 38/50, Loss: 0.4152\n",
            "[Transformer] Epoch 39/50, Loss: 0.5046\n",
            "[Transformer] Epoch 40/50, Loss: 0.3302\n",
            "[Transformer] Epoch 41/50, Loss: 0.3473\n",
            "[Transformer] Epoch 42/50, Loss: 0.4566\n",
            "[Transformer] Epoch 43/50, Loss: 0.3488\n",
            "[Transformer] Epoch 44/50, Loss: 0.3600\n",
            "[Transformer] Epoch 45/50, Loss: 0.3685\n",
            "[Transformer] Epoch 46/50, Loss: 0.2926\n",
            "[Transformer] Epoch 47/50, Loss: 0.3349\n",
            "[Transformer] Epoch 48/50, Loss: 0.2854\n",
            "[Transformer] Epoch 49/50, Loss: 0.3706\n",
            "[Transformer] Epoch 50/50, Loss: 0.3751\n"
          ]
        }
      ]
    }
  ]
}